{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTMS Model - Automated Configuration Search\n",
    "\n",
    "This notebook automatically searches for the best configuration by:\n",
    "1. Testing different numbers of CN samples for training\n",
    "2. Comparing without personalization (fixed weights) vs with personalization (adaptive weights)\n",
    "3. Saving the best configurations\n",
    "\n",
    "**Goal**: CN should have significantly fewer anomalies than CI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4090\n",
      "GPU Memory: 25.36 GB\n",
      "✓ CTMS modules imported successfully\n",
      "Configuration loaded\n",
      "Will test 8 training splits\n",
      "Will test 5 fixed weight configs\n",
      "Plus adaptive weight personalization\n",
      "✓ Loaded data\n",
      "  CN participants: 26\n",
      "  CI participants: 42\n",
      "  Total sequences: 128\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import copy\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set device - utilize RTX 4090\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Import CTMS modules\n",
    "from ctms_model_gpu import CTMSModelGPU\n",
    "\n",
    "print(\"✓ CTMS modules imported successfully\")\n",
    "\n",
    "\n",
    "# Data paths - UPDATE THESE\n",
    "SEQUENCE_FILE = '../sample_data/sequences.jsonl'\n",
    "LABEL_FILE = '../sample_data/subjects.json'\n",
    "\n",
    "# Search configuration\n",
    "SEARCH_CONFIG = {\n",
    "    'cn_train_ratios': [0.3, 0.5, 0.7, 0.8],  # Test different amounts of CN for training\n",
    "    'use_ci_in_train': [False, True],  # Try with/without CI in training\n",
    "    'ci_train_ratio': 0.3,  # If using CI in training, how much\n",
    "    'epochs_per_config': 30,  # Epochs for each configuration\n",
    "    'batch_size': 64,  # Larger batch for 4090\n",
    "    'learning_rate': 2e-4,\n",
    "}\n",
    "\n",
    "# Weight configurations\n",
    "WEIGHT_CONFIGS = {\n",
    "    'without_personalization': [\n",
    "        [0.25, 0.25, 0.25, 0.25],  # Equal weights\n",
    "        [0.4, 0.3, 0.2, 0.1],      # Emphasize circadian\n",
    "        [0.3, 0.4, 0.2, 0.1],      # Emphasize task\n",
    "        [0.3, 0.3, 0.3, 0.1],      # De-emphasize social\n",
    "        [0.35, 0.35, 0.2, 0.1],    # Balanced circadian+task\n",
    "    ],\n",
    "    'with_personalization': True  # Will be adaptive per person\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"Will test {len(SEARCH_CONFIG['cn_train_ratios']) * len(SEARCH_CONFIG['use_ci_in_train'])} training splits\")\n",
    "print(f\"Will test {len(WEIGHT_CONFIGS['without_personalization'])} fixed weight configs\")\n",
    "print(f\"Plus adaptive weight personalization\")\n",
    "\n",
    "\n",
    "def load_data(sequence_file, label_file):\n",
    "    \"\"\"Load sequences and labels\"\"\"\n",
    "    # Load sequences\n",
    "    sequences = defaultdict(list)\n",
    "    with open(sequence_file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            sequences[data['anon_id']].append(data)\n",
    "    \n",
    "    # Load labels\n",
    "    with open(label_file, 'r') as f:\n",
    "        label_data = json.load(f)\n",
    "        if isinstance(label_data, list):\n",
    "            labels = {item['anon_id']: item for item in label_data}\n",
    "        else:\n",
    "            labels = label_data\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "sequences, labels = load_data(SEQUENCE_FILE, LABEL_FILE)\n",
    "\n",
    "# Separate CN and CI\n",
    "cn_ids = [aid for aid, label in labels.items() if label['label'] == 'CN']\n",
    "ci_ids = [aid for aid, label in labels.items() if label['label'] == 'CI']\n",
    "\n",
    "print(f\"✓ Loaded data\")\n",
    "print(f\"  CN participants: {len(cn_ids)}\")\n",
    "print(f\"  CI participants: {len(ci_ids)}\")\n",
    "print(f\"  Total sequences: {sum(len(s) for s in sequences.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] df_all 已存在，跳过构建\n"
     ]
    }
   ],
   "source": [
    "# ----- Option B: 构建df_all（使用 CTMSModelGPU，从 sequences 推断 cdi/tir/me/sws）\n",
    "# 说明：可能较慢；默认只跑前 N 个参与者用于 smoke test，可修改 MAX_SUBJECTS=None 遍历全部\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "if 'df_all' not in globals():\n",
    "    print(\"[INFO] 构建 df_all：使用 CTMSModelGPU 从原始 sequences 中提取行为度量（默认前 50 个 subject）。\")\n",
    "    MAX_SUBJECTS = None  # 设置为 None 则遍历全部\n",
    "    subj_ids = list(sequences.keys())\n",
    "    if MAX_SUBJECTS is not None:\n",
    "        subj_ids = subj_ids[:MAX_SUBJECTS]\n",
    "\n",
    "    # 初始化模型（与 notebook 其它部分一致的超参）\n",
    "    model = CTMSModelGPU(d_model=128, num_activities=21, num_task_templates=20, use_fast_similarity=True)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    rows = []\n",
    "    for aid in tqdm(subj_ids, desc=\"subjects\"):\n",
    "        recs = sequences.get(aid, [])\n",
    "        activity_ids = []\n",
    "        timestamps = []\n",
    "        for rec in recs:\n",
    "            seq = rec.get('sequence', []) or rec.get('seq', [])\n",
    "            for ev in seq:\n",
    "                # 支持多种可能的字段名\n",
    "                act_field = None\n",
    "                for k in ('action_id','activity_id','activity','action'):\n",
    "                    if k in ev:\n",
    "                        act_field = ev.get(k)\n",
    "                        break\n",
    "                ts_field = None\n",
    "                for k in ('ts','timestamp','time'):\n",
    "                    if k in ev:\n",
    "                        ts_field = ev.get(k)\n",
    "                        break\n",
    "                if act_field is None:\n",
    "                    continue\n",
    "                try:\n",
    "                    activity_ids.append(int(act_field))\n",
    "                except Exception:\n",
    "                    # 若无法转为 int，跳过\n",
    "                    continue\n",
    "                timestamps.append(int(ts_field) if ts_field is not None else 0)\n",
    "\n",
    "        if len(activity_ids) == 0:\n",
    "            # 没有事件，跳过\n",
    "            print(f\"[WARN] subject {aid} has no events; skipping\")\n",
    "            continue\n",
    "\n",
    "        act_tensor = torch.tensor([activity_ids], dtype=torch.long, device=device)  # [1, seq_len]\n",
    "        ts_tensor  = torch.tensor([timestamps], dtype=torch.long, device=device)\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = model.forward(act_tensor, ts_tensor, return_encodings_only=False)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] model.forward failed for subject {aid}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 按照 ctms_model.py 的规范直接读取标准字段\n",
    "        try:\n",
    "            cdi = float(outputs['cdi'].detach().cpu().item()) if 'cdi' in outputs else float(outputs.get('CDI', 0.0))\n",
    "        except Exception:\n",
    "            cdi = float(outputs.get('cdi', 0.0))\n",
    "        try:\n",
    "            tir = float(outputs['tir'].detach().cpu().item()) if 'tir' in outputs else float(outputs.get('TIR', 0.0))\n",
    "        except Exception:\n",
    "            tir = float(outputs.get('tir', 0.0))\n",
    "        try:\n",
    "            me  = float(outputs['me'].detach().cpu().item()) if 'me' in outputs else float(outputs.get('ME', 0.0))\n",
    "        except Exception:\n",
    "            me = float(outputs.get('me', 0.0))\n",
    "        try:\n",
    "            sws = float(outputs['sws'].detach().cpu().item()) if 'sws' in outputs else float(outputs.get('SWS', 0.0))\n",
    "        except Exception:\n",
    "            sws = float(outputs.get('sws', 0.0))\n",
    "\n",
    "        rows.append({\n",
    "            'anon_id': aid,\n",
    "            'encoding': [cdi, tir, me, sws],\n",
    "            'circadian': float(cdi),\n",
    "            'task': float(tir),\n",
    "            'movement': float(me),\n",
    "            'social': float(sws),\n",
    "            'label': labels.get(aid, {}).get('label', 'CN'),\n",
    "            'label_binary': 0 if labels.get(aid, {}).get('label','CN')=='CN' else 1\n",
    "        })\n",
    "\n",
    "    df_all = pd.DataFrame(rows)\n",
    "    print(f\"[INFO] df_all 构建完成，n={len(df_all)}\")\n",
    "else:\n",
    "    print(\"[INFO] df_all 已存在，跳过构建\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WITHOUT personalization（全局最优）===\n",
      "         weights_name                                            weights  \\\n",
      "0          Movement++                               [0.1, 0.1, 0.7, 0.1]   \n",
      "1  Circadian-dominant                             [0.7, 0.2, 0.05, 0.05]   \n",
      "2               Equal                           [0.25, 0.25, 0.25, 0.25]   \n",
      "3            Social++                               [0.1, 0.1, 0.1, 0.7]   \n",
      "4    Grid_C0.50_T0.35  [0.5, 0.35, 0.07500000000000001, 0.07500000000...   \n",
      "5    Grid_C0.45_T0.40  [0.45, 0.39999999999999997, 0.0750000000000000...   \n",
      "6               CT+++                             [0.5, 0.4, 0.05, 0.05]   \n",
      "7    Grid_C0.50_T0.40  [0.5, 0.39999999999999997, 0.05000000000000004...   \n",
      "8         Circadian++                             [0.5, 0.35, 0.1, 0.05]   \n",
      "9         CT-Balanced                           [0.45, 0.45, 0.05, 0.05]   \n",
      "\n",
      "   threshold        f1       acc      prec      rec   p_value       sep  \n",
      "0   0.074354  0.854167  0.745455  0.759259  0.97619  0.599594  0.212505  \n",
      "1   0.045323  0.854167  0.745455  0.759259  0.97619  0.699268  0.008847  \n",
      "2   0.055951  0.854167  0.745455  0.759259  0.97619  0.713993  0.105122  \n",
      "3   0.035387  0.854167  0.745455  0.759259  0.97619  0.713993  0.066485  \n",
      "4   0.043277  0.854167  0.745455  0.759259  0.97619  0.728826  0.030051  \n",
      "5   0.041988  0.854167  0.745455  0.759259  0.97619  0.743761  0.032288  \n",
      "6   0.040184  0.854167  0.745455  0.759259  0.97619  0.743761  0.016617  \n",
      "7   0.040184  0.854167  0.745455  0.759259  0.97619  0.743761  0.016617  \n",
      "8   0.046163  0.854167  0.745455  0.759259  0.97619  0.789127  0.041467  \n",
      "9   0.038793  0.854167  0.745455  0.759259  0.97619  0.789127  0.018818  \n",
      "\n",
      "BEST (global):\n",
      "weights_name              Movement++\n",
      "weights         [0.1, 0.1, 0.7, 0.1]\n",
      "threshold                   0.074354\n",
      "f1                          0.854167\n",
      "acc                         0.745455\n",
      "prec                        0.759259\n",
      "rec                          0.97619\n",
      "p_value                     0.599594\n",
      "sep                         0.212505\n",
      "Name: 0, dtype: object\n",
      "\n",
      "=== WITH personalization (fixed_fpr, 无标签) ===\n",
      "{'method': 'fixed_fpr', 'alpha': 0.95, 'f1': 0.37037037037037035, 'acc': 0.38181818181818183, 'prec': 0.8333333333333334, 'rec': 0.23809523809523808, 'p_value': np.float64(1.5421197988598157e-07), 'weights_top_counts': {'Social++': 39, 'Movement++': 16}}\n",
      "\n",
      "=== WITH personalization (oracle, 上限) ===\n",
      "{'method': 'oracle', 'alpha': 0.975, 'f1': 0.865979381443299, 'acc': 0.7636363636363637, 'prec': 0.7636363636363637, 'rec': 1.0, 'p_value': nan, 'weights_top_counts': {'Movement++': 40, 'Circadian-dominant': 15}}\n",
      "\n",
      "✓ 保存文件：\n",
      " - without_personalization_global_scan.csv\n",
      " - with_personalization_zscore_detail.csv\n",
      " - with_personalization_oracle_detail.csv\n",
      "✓ Saved best_without_config.json\n",
      "✓ Saved best_with_config.json\n",
      "✓ Saved best_configs_summary.json\n",
      "[check] best_without_config.json: exists=True, size=6369 bytes\n",
      "[check] best_with_config.json: exists=True, size=26722 bytes\n",
      "[check] best_configs_summary.json: exists=True, size=36478 bytes\n",
      "\n",
      "=== Summary (for paper) ===\n",
      "               Mode   ACC    F1 Precision Recall/Sens. Specificity Bal.ACC    MCC\n",
      "   Without (global) 0.745 0.854     0.759        0.976       0.000   0.488 -0.076\n",
      "With (personalized) 0.382 0.346     0.900        0.214       0.923   0.569  0.151\n",
      "\n",
      "=== WITH personalization (oracle, 上限) ===\n",
      "{'method': 'oracle', 'alpha': 0.975, 'f1': 0.865979381443299, 'acc': 0.7636363636363637, 'prec': 0.7636363636363637, 'rec': 1.0, 'p_value': nan, 'weights_top_counts': {'Movement++': 40, 'Circadian-dominant': 15}}\n",
      "\n",
      "✓ 保存文件：\n",
      " - without_personalization_global_scan.csv\n",
      " - with_personalization_zscore_detail.csv\n",
      " - with_personalization_oracle_detail.csv\n",
      "✓ Saved best_without_config.json\n",
      "✓ Saved best_with_config.json\n",
      "✓ Saved best_configs_summary.json\n",
      "[check] best_without_config.json: exists=True, size=6369 bytes\n",
      "[check] best_with_config.json: exists=True, size=26722 bytes\n",
      "[check] best_configs_summary.json: exists=True, size=36478 bytes\n",
      "\n",
      "=== Summary (for paper) ===\n",
      "               Mode   ACC    F1 Precision Recall/Sens. Specificity Bal.ACC    MCC\n",
      "   Without (global) 0.745 0.854     0.759        0.976       0.000   0.488 -0.076\n",
      "With (personalized) 0.382 0.346     0.900        0.214       0.923   0.569  0.151\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 固定权重 vs 个性化权重：自动选最优配置（支持 oracle / zscore 两种策略）\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from itertools import product\n",
    "\n",
    "# ------------- 基础工具 -------------\n",
    "def split_train_test_cn(df, train_ratio=0.5, seed=123):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    cn = df[df['label']==\"CN\"].sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    ci = df[df['label']==\"CI\"].reset_index(drop=True)\n",
    "    n_train = max(1, int(len(cn)*train_ratio))\n",
    "    train_cn = cn.iloc[:n_train].copy()\n",
    "    test = pd.concat([cn.iloc[n_train:], ci], ignore_index=True)\n",
    "    return train_cn, test\n",
    "\n",
    "def center_std(train_cn):\n",
    "    c = np.array([train_cn['circadian'].mean(),\n",
    "                  train_cn['task'].mean(),\n",
    "                  train_cn['movement'].mean(),\n",
    "                  train_cn['social'].mean()])\n",
    "    s = np.array([train_cn['circadian'].std(ddof=1),\n",
    "                  train_cn['task'].std(ddof=1),\n",
    "                  train_cn['movement'].std(ddof=1),\n",
    "                  train_cn['social'].std(ddof=1)])\n",
    "    s[~np.isfinite(s)] = 1.0\n",
    "    s[s==0] = 1.0\n",
    "    return c, s\n",
    "\n",
    "def distances(df, center, std, w):\n",
    "    z = (np.stack(df['encoding'].to_list(), axis=0) - center) / std\n",
    "    w = np.asarray(w, float)\n",
    "    return np.sqrt((w*(z**2)).sum(axis=1))\n",
    "\n",
    "def scan_threshold(d, y, n=200):\n",
    "    d = np.asarray(d); y = np.asarray(y).astype(int)\n",
    "    ths = np.linspace(d.min(), d.max(), n)\n",
    "    best = {\"threshold\": None, \"f1\": -1, \"acc\": None,\n",
    "            \"prec\": None, \"rec\": None}\n",
    "    for th in ths:\n",
    "        yp = (d > th).astype(int)\n",
    "        if len(np.unique(yp)) < 2:  # 全同\n",
    "            continue\n",
    "        f1 = f1_score(y, yp)\n",
    "        acc = accuracy_score(y, yp)\n",
    "        prec = precision_score(y, yp, zero_division=0)\n",
    "        rec  = recall_score(y, yp, zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best.update({\"threshold\": th, \"f1\": f1, \"acc\": acc, \"prec\": prec, \"rec\": rec})\n",
    "    return best\n",
    "\n",
    "def mannwhitney_p(cn_d, ci_d):\n",
    "    try:\n",
    "        _, p = stats.mannwhitneyu(cn_d, ci_d, alternative=\"two-sided\")\n",
    "    except Exception:\n",
    "        p = np.nan\n",
    "    return p\n",
    "\n",
    "# ------------- 权重候选集合 -------------\n",
    "def make_weight_candidates():\n",
    "    # 预设若干“极端/偏置”组合 LLM 推荐\n",
    "    presets = [\n",
    "        (\"Equal\",                [0.25, 0.25, 0.25, 0.25]),\n",
    "        (\"Circadian++\",          [0.50, 0.35, 0.10, 0.05]),\n",
    "        (\"Task++\",               [0.35, 0.50, 0.10, 0.05]),\n",
    "        (\"Circadian-dominant\",   [0.70, 0.20, 0.05, 0.05]),\n",
    "        (\"Task-dominant\",        [0.20, 0.70, 0.05, 0.05]),\n",
    "        (\"Movement++\",           [0.10, 0.10, 0.70, 0.10]),\n",
    "        (\"Social++\",             [0.10, 0.10, 0.10, 0.70]),\n",
    "        (\"CT+++\",                [0.50, 0.40, 0.05, 0.05]),\n",
    "        (\"CT-Balanced\",          [0.45, 0.45, 0.05, 0.05]),\n",
    "    ]\n",
    "    # 网格细化（强化 C/T，M/S 均分剩余）\n",
    "    name2w = {n: np.array(w, float) for n, w in presets}\n",
    "    for c, t in product(np.arange(0.45, 0.81, 0.05), np.arange(0.35, 0.71, 0.05)):\n",
    "        rest = 1.0 - (c + t)\n",
    "        if rest < 0.10:  # 给 M+S 至少 0.10\n",
    "            continue\n",
    "        m = s = rest/2\n",
    "        if m < 0.05 or s < 0.05:\n",
    "            continue\n",
    "        name2w[f\"Grid_C{c:.2f}_T{t:.2f}\"] = np.array([c, t, m, s], float)\n",
    "    return name2w\n",
    "\n",
    "# ------------- without personalization：全局最优 -------------\n",
    "def evaluate_global(df, train_ratio=0.5, seed=123, candidates=None):\n",
    "    if candidates is None:\n",
    "      candidates = make_weight_candidates()\n",
    "    train_cn, test = split_train_test_cn(df, train_ratio, seed)\n",
    "    c, s = center_std(train_cn)\n",
    "    y = test['label_binary'].values\n",
    "\n",
    "    rows = []\n",
    "    context = {}\n",
    "    for name, w in candidates.items():\n",
    "        d = distances(test, c, s, w)\n",
    "        best = scan_threshold(d, y)\n",
    "        cn_d = d[test['label'].values=='CN']; ci_d = d[test['label'].values=='CI']\n",
    "        rows.append({\n",
    "            \"weights_name\": name, \"weights\": w.tolist(),\n",
    "            \"threshold\": best[\"threshold\"],\n",
    "            \"f1\": best[\"f1\"], \"acc\": best[\"acc\"],\n",
    "            \"prec\": best[\"prec\"], \"rec\": best[\"rec\"],\n",
    "            \"p_value\": mannwhitney_p(cn_d, ci_d),\n",
    "            \"sep\": float(ci_d.mean() - cn_d.mean())\n",
    "        })\n",
    "        context[name] = {\"d\": d, \"y\": y, \"center\": c, \"std\": s, \"threshold\": best[\"threshold\"]}\n",
    "    res = pd.DataFrame(rows).sort_values([\"p_value\",\"sep\"], ascending=[True, False]).reset_index(drop=True)\n",
    "    best_row = res.iloc[0]\n",
    "    return {\"results\": res, \"best\": best_row, \"context\": context, \"train_cn\": train_cn, \"test\": test}\n",
    "\n",
    "# ------------- with personalization：为每个人挑权重 -------------\n",
    "def evaluate_personalized(df, train_ratio=0.5, seed=123, candidates=None,\n",
    "                          method=\"fixed_fpr\", alpha=0.975,\n",
    "                          global_fallback=None):\n",
    "    \"\"\"\n",
    "    method:\n",
    "      - 'fixed_fpr'：无标签。每个权重用训练CN的分位数阈值；为每个样本选 margin 最大的权重；\n",
    "                     预测= (max_margin>0)。支持回退到全局最优（可选）。\n",
    "      - 'oracle'   ：上限评估（与之前一致）。\n",
    "    global_fallback: 传入一个 dict，形如\n",
    "        {\"weights\": np.array([...]), \"threshold\": float, \"center\": c, \"std\": s}\n",
    "      可用 evaluate_global(...) 的 best 配置构建；为空则不回退。\n",
    "    \"\"\"\n",
    "    if candidates is None:\n",
    "        candidates = make_weight_candidates()\n",
    "\n",
    "    train_cn, test = split_train_test_cn(df, train_ratio, seed)\n",
    "    c, s = center_std(train_cn)\n",
    "    y = test['label_binary'].values\n",
    "    N = len(test)\n",
    "\n",
    "    # 预计算每个权重的距离 & 阈值\n",
    "    per_weight = {}\n",
    "    for name, w in candidates.items():\n",
    "        d_test  = distances(test, c, s, w)\n",
    "        d_train = distances(train_cn, c, s, w)\n",
    "        if method == \"oracle\":\n",
    "            th = scan_threshold(d_test, y)[\"threshold\"]\n",
    "        else:  # fixed_fpr\n",
    "            th = np.quantile(d_train, alpha)  # 控 CN 的假阳率\n",
    "        per_weight[name] = {\"w\": w, \"d_test\": d_test, \"th\": th}\n",
    "\n",
    "    # （可选）全局回退\n",
    "    gf = None\n",
    "    if global_fallback is not None:\n",
    "        w_g  = np.array(global_fallback[\"weights\"], float)\n",
    "        th_g = float(global_fallback[\"threshold\"])\n",
    "        c_g  = np.array(global_fallback[\"center\"], float)\n",
    "        s_g  = np.array(global_fallback[\"std\"], float)\n",
    "        d_g  = distances(test, c_g, s_g, w_g)\n",
    "        gf = {\"d\": d_g, \"th\": th_g}\n",
    "\n",
    "    # 逐人选择权重：最大 margin\n",
    "    chosen = []\n",
    "    y_pred = np.zeros(N, dtype=int)\n",
    "    margins = np.zeros(N, dtype=float)\n",
    "    for i in range(N):\n",
    "        best_name, best_margin, best_pred = None, -np.inf, 0\n",
    "        for name, obj in per_weight.items():\n",
    "            d_i  = obj[\"d_test\"][i]\n",
    "            m_i  = d_i - obj[\"th\"]     # >0 说明“超阈值”\n",
    "            pred = int(m_i > 0.0)\n",
    "            if m_i > best_margin:\n",
    "                best_name, best_margin, best_pred = name, m_i, pred\n",
    "\n",
    "        # 回退逻辑（可选）：若所有 margin<=0 且全局能给出正margin，就用全局\n",
    "        if gf is not None and best_margin <= 0.0:\n",
    "            m_g = gf[\"d\"][i] - gf[\"th\"]\n",
    "            if m_g > best_margin:\n",
    "                best_name, best_margin, best_pred = \"GLOBAL_FALLBACK\", m_g, int(m_g > 0.0)\n",
    "\n",
    "        chosen.append(best_name)\n",
    "        y_pred[i] = best_pred\n",
    "        margins[i] = best_margin\n",
    "\n",
    "    # 汇总指标\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "    f1  = f1_score(y, y_pred)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    prec = precision_score(y, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y, y_pred, zero_division=0)\n",
    "\n",
    "    # 统计显著性（用被选权重的距离）\n",
    "    d_chosen = np.array([\n",
    "        (gf[\"d\"][i] if chosen[i]==\"GLOBAL_FALLBACK\" else per_weight[chosen[i]][\"d_test\"][i])\n",
    "        for i in range(N)\n",
    "    ])\n",
    "    cn_mask, ci_mask = (y_pred==0), (y_pred==1)\n",
    "    try:\n",
    "        from scipy import stats\n",
    "        p_val = stats.mannwhitneyu(d_chosen[cn_mask], d_chosen[ci_mask], alternative=\"two-sided\").pvalue \\\n",
    "                if cn_mask.any() and ci_mask.any() else np.nan\n",
    "    except Exception:\n",
    "        p_val = np.nan\n",
    "\n",
    "    summary = {\n",
    "        \"method\": method,\n",
    "        \"alpha\": alpha,\n",
    "        \"f1\": f1, \"acc\": acc, \"prec\": prec, \"rec\": rec,\n",
    "        \"p_value\": p_val,\n",
    "        \"weights_top_counts\": pd.Series(chosen).value_counts().to_dict()\n",
    "    }\n",
    "    detail = pd.DataFrame({\n",
    "        \"anon_id\": test[\"anon_id\"].values if \"anon_id\" in test.columns else np.arange(N),\n",
    "        \"true\": y, \"pred\": y_pred,\n",
    "        \"chosen_weight\": chosen, \"margin\": margins\n",
    "    })\n",
    "    return summary, detail, {\"train_cn\": train_cn, \"test\": test, \"per_weight\": per_weight, \"global_fallback\": gf}\n",
    "\n",
    "# ===================== 一键运行 =====================\n",
    "# 1) 全局固定权重（without personalization）\n",
    "global_out = evaluate_global(df_all, train_ratio=0.5, seed=123)\n",
    "print(\"\\n=== WITHOUT personalization（全局最优）===\")\n",
    "print(global_out[\"results\"].head(10))  # Top10\n",
    "print(\"\\nBEST (global):\")\n",
    "print(global_out[\"best\"])\n",
    "\n",
    "# 2) 个性化权重（with personalization）\n",
    "# 2.1 无泄露（zscore）\n",
    "pers_z_summary, pers_z_detail, _ = evaluate_personalized(df_all, train_ratio=0.5, seed=123, method=\"fixed_fpr\", alpha=0.95)\n",
    "print(\"\\n=== WITH personalization (fixed_fpr, 无标签) ===\")\n",
    "print(pers_z_summary)\n",
    "# 可保存\n",
    "pers_z_detail.to_csv(\"with_personalization_zscore_detail.csv\", index=False)\n",
    "\n",
    "# 2.2 上限评估（oracle）\n",
    "pers_orc_summary, pers_orc_detail, _ = evaluate_personalized(df_all, train_ratio=0.5, seed=123, method=\"oracle\")\n",
    "print(\"\\n=== WITH personalization (oracle, 上限) ===\")\n",
    "print(pers_orc_summary)\n",
    "# 可保存\n",
    "pers_orc_detail.to_csv(\"with_personalization_oracle_detail.csv\", index=False)\n",
    "\n",
    "# 3) 也把全局结果保存一下\n",
    "global_out[\"results\"].to_csv(\"without_personalization_global_scan.csv\", index=False)\n",
    "print(\"\\n✓ 保存文件：\")\n",
    "print(\" - without_personalization_global_scan.csv\")\n",
    "print(\" - with_personalization_zscore_detail.csv\")\n",
    "print(\" - with_personalization_oracle_detail.csv\")\n",
    "\n",
    "# ============================================================\n",
    "# 保存 best without / best with（含 Train/Test 划分与逐人权重）\n",
    "# 替换你之前的保存 JSON 代码段\n",
    "# ============================================================\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef\n",
    "# ===== 放在保存 JSON 段的最上面：通用“去NumPy化”工具 =====\n",
    "\n",
    "import os, json, numpy as np, pandas as pd\n",
    "\n",
    "def to_builtin(o):\n",
    "    if isinstance(o, dict):\n",
    "        return {to_builtin(k): to_builtin(v) for k, v in o.items()}\n",
    "    if isinstance(o, (list, tuple, set)):\n",
    "        return [to_builtin(x) for x in o]\n",
    "    if isinstance(o, (np.integer,)):   return int(o)\n",
    "    if isinstance(o, (np.floating,)):  return float(o)\n",
    "    if isinstance(o, (np.bool_,)):     return bool(o)\n",
    "    if isinstance(o, (np.ndarray,)):   return o.tolist()\n",
    "    if isinstance(o, pd.Series):       return o.tolist()\n",
    "    if isinstance(o, pd.DataFrame):    return o.to_dict(orient=\"records\")\n",
    "    return o\n",
    "\n",
    "def json_dump_safe(obj, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(to_builtin(obj), f, indent=2)\n",
    "\n",
    "# ---- 工具：富指标 ----\n",
    "def compute_metrics_rich(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    acc = (tp + tn) / max(1, (tp + tn + fp + fn))\n",
    "    prec = tp / max(1, (tp + fp))\n",
    "    rec = tp / max(1, (tp + fn))                     # sensitivity / recall\n",
    "    spec = tn / max(1, (tn + fp))                    # specificity\n",
    "    f1 = (2*prec*rec / (prec + rec)) if (prec + rec) > 0 else 0.0\n",
    "    bal_acc = 0.5 * (rec + spec)\n",
    "    try:\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        if not np.isfinite(mcc): mcc = 0.0\n",
    "    except Exception:\n",
    "        mcc = 0.0\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall_sensitivity\": float(rec),\n",
    "        \"specificity\": float(spec),\n",
    "        \"f1\": float(f1),\n",
    "        \"balanced_accuracy\": float(bal_acc),\n",
    "        \"mcc\": float(mcc),\n",
    "        \"confusion_matrix\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)},\n",
    "        \"fpr\": float(1.0 - spec),\n",
    "        \"fnr\": float(1.0 - rec),\n",
    "        \"tpr\": float(rec),\n",
    "        \"tnr\": float(spec),\n",
    "    }\n",
    "\n",
    "# ---------------- best WITHOUT ----------------\n",
    "_best_row = global_out[\"best\"]\n",
    "_best_name = _best_row[\"weights_name\"]\n",
    "_ctx = global_out[\"context\"][_best_name]\n",
    "\n",
    "y_true_wo = _ctx[\"y\"]\n",
    "dists_wo  = _ctx[\"d\"]\n",
    "th_wo     = float(_ctx[\"threshold\"])\n",
    "y_pred_wo = (dists_wo > th_wo).astype(int)\n",
    "metrics_wo = compute_metrics_rich(y_true_wo, y_pred_wo)\n",
    "\n",
    "# 记录划分（train_cn/test）\n",
    "train_cn_df = global_out[\"train_cn\"]\n",
    "test_df     = global_out[\"test\"]\n",
    "\n",
    "best_without = {\n",
    "    \"config\": {\n",
    "        \"type\": \"without\",\n",
    "        \"cn_train_ratio\": 0.5,                 # 如你改了比例，记得同步\n",
    "        \"use_ci_in_train\": False,\n",
    "        \"seed\": 123,\n",
    "    },\n",
    "    \"data_split\": {\n",
    "        \"train_cn_ids\": train_cn_df[\"anon_id\"].tolist() if \"anon_id\" in train_cn_df.columns else [],\n",
    "        \"test\": [\n",
    "            {\n",
    "                \"anon_id\": row[\"anon_id\"] if \"anon_id\" in test_df.columns else int(i),\n",
    "                \"label\": row[\"label\"] if \"label\" in test_df.columns else None,\n",
    "                \"label_binary\": int(row[\"label_binary\"]) if \"label_binary\" in test_df.columns else None,\n",
    "            }\n",
    "            for i, row in test_df.reset_index(drop=True).iterrows()\n",
    "        ],\n",
    "    },\n",
    "    \"decision\": {\n",
    "        \"weights_name\": str(_best_row[\"weights_name\"]),\n",
    "        \"weights\": list(map(float, _best_row[\"weights\"])),\n",
    "        \"threshold\": float(th_wo),\n",
    "        \"center\": list(map(float, _ctx[\"center\"])),\n",
    "        \"std\": list(map(float, _ctx[\"std\"])),\n",
    "    },\n",
    "    \"score\": float(_best_row[\"f1\"]),  # 你之前用F1作为主score\n",
    "    \"metrics\": {\n",
    "        \"scan\": {\n",
    "            \"f1\": float(_best_row[\"f1\"]),\n",
    "            \"accuracy\": float(_best_row[\"acc\"]),\n",
    "            \"precision\": float(_best_row[\"prec\"]),\n",
    "            \"recall\": float(_best_row[\"rec\"]),\n",
    "            \"p_value_mannwhitney\": float(_best_row[\"p_value\"]) if pd.notnull(_best_row[\"p_value\"]) else None,\n",
    "            \"separation_mean_ci_minus_cn\": float(_best_row[\"sep\"]),\n",
    "        },\n",
    "        \"final\": metrics_wo  # 基于最终阈值的混淆矩阵等\n",
    "    }\n",
    "}\n",
    "\n",
    "json_dump_safe(best_without, \"best_without_config.json\")\n",
    "print(\"✓ Saved best_without_config.json\")\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- best WITH (fixed_fpr 无标签) ----------------\n",
    "# 重要：把第三个返回值接出来，拿到 per_weight 阈值与全局回退\n",
    "pers_z_summary, pers_z_detail, with_ctx = evaluate_personalized(\n",
    "    df_all, train_ratio=0.5, seed=123, method=\"fixed_fpr\", alpha=0.975\n",
    ")\n",
    "\n",
    "y_true_w  = pers_z_detail[\"true\"].values\n",
    "y_pred_w  = pers_z_detail[\"pred\"].values\n",
    "metrics_w = compute_metrics_rich(y_true_w, y_pred_w)\n",
    "\n",
    "# 每个权重的阈值字典（便于复现）\n",
    "per_weight = with_ctx[\"per_weight\"]           # {name: {\"w\":..., \"d_test\":..., \"th\":...}}\n",
    "thresholds_per_weight = {\n",
    "    name: float(obj[\"th\"]) for name, obj in per_weight.items()\n",
    "}\n",
    "weights_vectors = {\n",
    "    name: list(map(float, obj[\"w\"])) for name, obj in per_weight.items()\n",
    "}\n",
    "\n",
    "# 逐人条目：chosen_weight / 距离 / 阈值 / margin / 真值/预测\n",
    "# pers_z_detail 里已含 chosen_weight、margin、true、pred\n",
    "# 距离与阈值需要从 per_weight 或 gf 中取\n",
    "gf = with_ctx.get(\"global_fallback\", None)\n",
    "\n",
    "per_subject = []\n",
    "# 需要与 test 对齐 anon_id/label\n",
    "test_df_w = with_ctx[\"test\"].reset_index(drop=True)\n",
    "for i, row in pers_z_detail.reset_index(drop=True).iterrows():\n",
    "    chosen_name = row[\"chosen_weight\"]\n",
    "    if chosen_name == \"GLOBAL_FALLBACK\" and gf is not None:\n",
    "        # 全局回退：距离/阈值来自gf\n",
    "        dist_i = float(gf[\"d\"][i])\n",
    "        th_i   = float(gf[\"th\"])\n",
    "        w_vec  = None  # 全局已在 without JSON 里给出\n",
    "    else:\n",
    "        dist_i = float(per_weight[chosen_name][\"d_test\"][i])\n",
    "        th_i   = float(per_weight[chosen_name][\"th\"])\n",
    "        w_vec  = weights_vectors.get(chosen_name, None)\n",
    "\n",
    "    per_subject.append({\n",
    "        \"anon_id\": test_df_w[\"anon_id\"].iloc[i] if \"anon_id\" in test_df_w.columns else int(i),\n",
    "        \"label\": test_df_w[\"label\"].iloc[i] if \"label\" in test_df_w.columns else None,\n",
    "        \"true\": int(row[\"true\"]),\n",
    "        \"pred\": int(row[\"pred\"]),\n",
    "        \"chosen_weight_name\": chosen_name,\n",
    "        \"chosen_weight_vector\": w_vec,    # GLOBAL_FALLBACK 为 None\n",
    "        \"threshold_used\": th_i,\n",
    "        \"distance\": dist_i,\n",
    "        \"margin\": float(row[\"margin\"]),\n",
    "    })\n",
    "\n",
    "# 统计最常被选的权重\n",
    "weights_top_counts = pers_z_summary.get(\"weights_top_counts\", {})\n",
    "\n",
    "best_with = {\n",
    "    \"config\": {\n",
    "        \"type\": \"with\",\n",
    "        \"selection\": \"fixed_fpr\",\n",
    "        \"alpha\": float(pers_z_summary.get(\"alpha\", 0.975)),\n",
    "        \"cn_train_ratio\": 0.5,\n",
    "        \"use_ci_in_train\": False,\n",
    "        \"seed\": 123,\n",
    "        \"global_fallback_used\": gf is not None,\n",
    "    },\n",
    "    \"data_split\": {\n",
    "        \"train_cn_ids\": with_ctx[\"train_cn\"][\"anon_id\"].tolist() if \"anon_id\" in with_ctx[\"train_cn\"].columns else [],\n",
    "        \"test\": [\n",
    "            {\n",
    "                \"anon_id\": r[\"anon_id\"] if \"anon_id\" in with_ctx[\"test\"].columns else int(i),\n",
    "                \"label\": r[\"label\"] if \"label\" in with_ctx[\"test\"].columns else None,\n",
    "                \"label_binary\": int(r[\"label_binary\"]) if \"label_binary\" in with_ctx[\"test\"].columns else None,\n",
    "            }\n",
    "            for i, r in with_ctx[\"test\"].reset_index(drop=True).iterrows()\n",
    "        ],\n",
    "    },\n",
    "    \"thresholds_per_weight\": thresholds_per_weight,  # 每个权重对应的CN分位阈值\n",
    "    \"weights_library\": weights_vectors,              # 每个权重的向量（便于复现）\n",
    "    \"per_subject\": per_subject,                      # ★ 逐人设置（你关心的）\n",
    "    \"score\": float(pers_z_summary[\"f1\"]),\n",
    "    \"metrics\": {\n",
    "        \"summary\": {\n",
    "            \"f1\": float(pers_z_summary[\"f1\"]),\n",
    "            \"accuracy\": float(pers_z_summary[\"acc\"]),\n",
    "            \"precision\": float(pers_z_summary[\"prec\"]),\n",
    "            \"recall\": float(pers_z_summary[\"rec\"]),\n",
    "            \"p_value_mannwhitney\": float(pers_z_summary[\"p_value\"]) if pd.notnull(pers_z_summary[\"p_value\"]) else None,\n",
    "            \"weights_top_counts\": weights_top_counts,\n",
    "        },\n",
    "        \"final\": metrics_w\n",
    "    }\n",
    "}\n",
    "\n",
    "json_dump_safe(best_with, \"best_with_config.json\")  # ← 确保这行存在\n",
    "print(\"✓ Saved best_with_config.json\")  \n",
    "\n",
    "json_dump_safe({\"best_without\": best_without, \"best_with\": best_with}, \"best_configs_summary.json\")\n",
    "print(\"✓ Saved best_configs_summary.json\")\n",
    "\n",
    "# ---------- 立即校验 ----------\n",
    "for p in [\"best_without_config.json\",\"best_with_config.json\",\"best_configs_summary.json\"]:\n",
    "    print(f\"[check] {p}: exists={os.path.exists(p)}, size={os.path.getsize(p) if os.path.exists(p) else 0} bytes\")\n",
    "\n",
    "# 控制台简表（方便拷论文）\n",
    "def short_row(tag, m):\n",
    "    return {\n",
    "        \"Mode\": tag,\n",
    "        \"ACC\": f\"{m['accuracy']:.3f}\",\n",
    "        \"F1\": f\"{m['f1']:.3f}\",\n",
    "        \"Precision\": f\"{m['precision']:.3f}\",\n",
    "        \"Recall/Sens.\": f\"{m['recall_sensitivity']:.3f}\",\n",
    "        \"Specificity\": f\"{m['specificity']:.3f}\",\n",
    "        \"Bal.ACC\": f\"{m['balanced_accuracy']:.3f}\",\n",
    "        \"MCC\": f\"{m['mcc']:.3f}\",\n",
    "    }\n",
    "\n",
    "print(\"\\n=== Summary (for paper) ===\")\n",
    "print(pd.DataFrame([\n",
    "    short_row(\"Without (global)\", best_without[\"metrics\"][\"final\"]),\n",
    "    short_row(\"With (personalized)\", best_with[\"metrics\"][\"final\"]),\n",
    "]).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
